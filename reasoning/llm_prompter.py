"""
LLM Prompter Module
Interface for Large Language Model queries
"""

from typing import Dict, Optional, Tuple
import json
import os


class LLMPrompter:
    """Interface for querying LLMs"""
    
    def __init__(self, gpt_version: str = "gpt-3.5-turbo", api_key: Optional[str] = None,
                 base_url: Optional[str] = None):
        self.gpt_version = gpt_version
        self.api_key = api_key or os.getenv("OPENAI_API_KEY", "")
        self.base_url = base_url
        
        # Load prompt templates
        self.prompts = self._load_prompts()
    
    def _load_prompts(self) -> Dict:
        """Load prompt templates"""
        # Default prompts
        prompts = {
            'subgoal-verifier': {
                'template-system': 'You are a robot task analyzer. Verify if a subgoal was successfully completed and provide detailed explanations.',
                'template-user': 'Task: {task}\nSubgoal: {subgoal}\nObservation: {observation}\n\nWas the subgoal successful? Please answer:\n1. Yes or No\n2. A detailed explanation of why it succeeded or failed.\n\nIf it failed, explain what is missing or what went wrong based on the observation.'
            },
            'reasoning-execution': {
                'template-system': 'You are a robot failure analyzer. Explain why a robot action failed.',
                'template-user': 'Task: {task}\nAction: {action}\nObservation: {observation}\nWhy did the action fail?'
            },
            'reasoning-planning': {
                'template-system': 'You are a robot planner analyzer. Identify planning errors.',
                'template-user': 'Task: {task}\nPlan: {plan}\nFinal State: {final_state}\nExpected Goal: {goal}\nWhat went wrong in the plan?'
            },
            'constraint-generator': {
                'template-system': 'You are a constraint generator for robot tasks. Generate logical constraints based on scene graphs and task requirements.',
                'template-user': 'Task: {task}\nScene Graph: {scene_graph}\nTask Goal: {goal}\nGenerate logical constraints that must be satisfied for this task. Format: constraint_description (constraint_condition)'
            },
            'causal-verifier': {
                'template-system': 'You are a causal logic verifier. Verify if the causal relationships in a scene graph are logically consistent.',
                'template-user': 'Scene Graph: {scene_graph}\nAction: {action}\nExpected Effect: {expected_effect}\nActual Observation: {observation}\nVerify if the causal logic is consistent. Explain any inconsistencies.'
            },
            'consistency-verifier': {
                'template-system': 'You are a scene consistency verifier. Check if scene graphs at different time points are consistent.',
                'template-user': 'Previous Scene: {previous_scene}\nCurrent Scene: {current_scene}\nAction Performed: {action}\nCheck if the scene transition is consistent. Identify any inconsistencies.'
            }
        }
        return prompts
    
    def query(self, system_prompt: str, user_prompt: str, max_tokens: int = 500) -> Tuple[str, Dict]:
        """
        Query the LLM
        
        Args:
            system_prompt: System prompt
            user_prompt: User prompt
            max_tokens: Maximum tokens in response
            
        Returns:
            (response_text, metadata) tuple
        """
        if not self.api_key:
            # Mock response for testing
            return "Mock LLM response: The action failed because the target object was not found.", {}
        
        try:
            import openai
            
            # Configure client with base_url if provided (for poloapi or other providers)
            client_kwargs = {
                "api_key": self.api_key,
                "timeout": 60.0  # 60 seconds timeout to prevent hanging
            }
            if self.base_url:
                client_kwargs["base_url"] = self.base_url
            
            client = openai.OpenAI(**client_kwargs)
            
            response = client.chat.completions.create(
                model=self.gpt_version,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=max_tokens,
                temperature=0.7
            )
            
            response_text = response.choices[0].message.content
            metadata = {
                "model": response.model,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            }
            return response_text, metadata
            
        except ImportError:
            print("âš ï¸  openai package not installed. Install with: pip install openai")
            return "Mock LLM response (openai not installed)", {}
        except Exception as e:
            error_msg = str(e)
            print(f"âš ï¸  Error calling LLM API: {error_msg}")
            # å¦‚æžœæ˜¯è¶…æ—¶é”™è¯¯ï¼Œæä¾›æ›´è¯¦ç»†çš„æç¤º
            if "timeout" in error_msg.lower() or "timed out" in error_msg.lower():
                print("   ðŸ’¡ Tip: If using poloapi, make sure API_KEY and POLOAPI_BASE_URL are correctly set")
                print("   ðŸ’¡ Tip: Check your network connection and try again")
            return f"Mock LLM response (API error: {error_msg})", {}
    
    def verify_subgoal(self, task: str, subgoal: str, observation: str) -> Tuple[bool, str]:
        """
        Verify if a subgoal was successfully completed
        
        Returns:
            (is_success, explanation) tuple
        """
        prompt_info = self.prompts['subgoal-verifier']
        user_prompt = prompt_info['template-user'].format(
            task=task,
            subgoal=subgoal,
            observation=observation
        )
        
        response, _ = self.query(
            prompt_info['template-system'], 
            user_prompt,
            max_tokens=300  # Increase tokens for detailed explanation
        )
        
        is_success = "yes" in response.lower()
        
        # If failed and explanation is too brief, get more detailed explanation
        if not is_success and len(response.strip()) < 50:
            # Use execution failure explanation for more details
            detailed_explanation = self.explain_execution_failure(
                task=task,
                action=subgoal,
                observation=observation
            )
            # Combine the verification result with detailed explanation
            explanation = f"{response.strip()}\n\nDetailed Analysis: {detailed_explanation}"
            return is_success, explanation
        
        return is_success, response
    
    def explain_execution_failure(self, task: str, action: str, observation: str) -> str:
        """Explain why an execution action failed"""
        prompt_info = self.prompts['reasoning-execution']
        user_prompt = prompt_info['template-user'].format(
            task=task,
            action=action,
            observation=observation
        )
        
        response, _ = self.query(prompt_info['template-system'], user_prompt)
        return response
    
    def explain_planning_failure(self, task: str, plan: str, final_state: str, goal: str) -> str:
        """Explain why a planning approach failed"""
        prompt_info = self.prompts['reasoning-planning']
        user_prompt = prompt_info['template-user'].format(
            task=task,
            plan=plan,
            final_state=final_state,
            goal=goal
        )
        
        response, _ = self.query(prompt_info['template-system'], user_prompt)
        return response

